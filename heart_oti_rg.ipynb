{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "heart_oti_rg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Otimizando os parâmetros de regressão Logistica"
      ],
      "metadata": {
        "id": "hDPe_aiju70U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F7xwotgUuONj"
      },
      "outputs": [],
      "source": [
        "# Importando os módulos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.model_selection import cross_validate\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV , RandomizedSearchCV "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando o dataset\n",
        "heart_failure = pd.read_csv('heart_failure_clinical_records_dataset.csv')"
      ],
      "metadata": {
        "id": "L14oEo2YvU5F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fazendo as separação dos atributos e do target\n",
        "x = heart_failure.drop(['DEATH_EVENT'],axis=1).values\n",
        "y = heart_failure.DEATH_EVENT.values\n",
        "\n",
        "# Normalizando os dados\n",
        "x_std = StandardScaler().fit_transform(x)"
      ],
      "metadata": {
        "id": "Sjectd5jva0T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usando o metos undersampling para tornar os dados menos enviesados\n",
        "nm = RandomUnderSampler(random_state = 10)\n",
        "\n",
        "x_nm, y_nm = nm.fit_resample(x_std, y)"
      ],
      "metadata": {
        "id": "LNvuptZxvgLG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicando o gridsearch para otimizar os hiperparametros"
      ],
      "metadata": {
        "id": "vupaanocvzs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parametros = {'penalty':['l1', 'l2', 'elasticnet',None],'dual':[True,False],'fit_intercept':[True,False],\n",
        "              'max_iter':np.arange(21,31,1),'multi_class':['auto','ovr','multinomial'],}"
      ],
      "metadata": {
        "id": "dSpoCccCvmPf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(estimator=LogisticRegression(),param_grid=parametros)\n",
        "grid_search.fit(x_nm,y_nm)\n",
        "melhores_parametros = grid_search.best_params_\n",
        "melhor_resultado = grid_search.best_score_\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALwd7Wo5yu1x",
        "outputId": "ad9284a4-f535-4824-9cf1-67128b4dfb5c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "2100 fits failed out of a total of 2400.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "600 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "300 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 453, in _check_solver\n",
            "    \"Solver %s supports only dual=False, got dual=%s\" % (solver, dual)\n",
            "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "600 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "600 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
            "    % (all_penalties, penalty)\n",
            "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.72807018        nan        nan        nan 0.72807018\n",
            "        nan        nan        nan 0.72807018        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan\n",
            "        nan 0.7074224         nan        nan        nan 0.7074224\n",
            "        nan        nan        nan 0.70215924        nan        nan]\n",
            "  category=UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(melhores_parametros)\n",
        "print(melhor_resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnIRb6LHzMd1",
        "outputId": "0a407341-19d4-4120-a734-9243d775d0a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dual': False, 'fit_intercept': True, 'max_iter': 21, 'multi_class': 'auto', 'penalty': 'l2'}\n",
            "0.7280701754385965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "de 1 a 10 em max iter temos o seguinte resultado:\n",
        "\n",
        "{'dual': False, 'fit_intercept': True, 'max_iter': 5, 'multi_class': 'auto', 'penalty': 'l2'}\n",
        " com score de 0.7333333333333333\n",
        "\n",
        " que foi o melhor em que foi gerado."
      ],
      "metadata": {
        "id": "cXIkqmJpzXpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nome_metricas = ['accuracy', 'precision_macro', 'recall_macro']\n",
        "\n",
        "lg = LogisticRegression(dual=False,fit_intercept=True,max_iter=5,multi_class='auto',penalty='l2')\n",
        "\n",
        "metricas_ran = cross_validate(lg,x_nm, y_nm, cv=7, scoring=nome_metricas)\n",
        "for met in metricas_ran:\n",
        "  print(f'-{met}')\n",
        "  print(f\"-- {metricas_ran[met]}\")\n",
        "  media = np.mean(metricas_ran[met])\n",
        "  desvio = np.std(metricas_ran[met])\n",
        "  print(f'Média do {met}: {media}')\n",
        "  print(f'Desvio {desvio}')\n",
        "  print(f'Intervalo [{(media-(2*desvio)):.3f},{(media+(2*desvio)):.3f}]')\n",
        "  print('-*-'*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9BQFFCvz_Qk",
        "outputId": "ab0aaa80-5245-44fb-998d-d72136fc2c72"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-fit_time\n",
            "-- [0.00714111 0.00447822 0.00398111 0.00284123 0.00264049 0.00260234\n",
            " 0.00261903]\n",
            "Média do fit_time: 0.0037576471056256977\n",
            "Desvio 0.0015462421251022577\n",
            "Intervalo [0.001,0.007]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
            "-score_time\n",
            "-- [0.00313282 0.00287366 0.00191998 0.00181985 0.00168538 0.00177002\n",
            " 0.0018146 ]\n",
            "Média do score_time: 0.002145188195364816\n",
            "Desvio 0.000550857665173927\n",
            "Intervalo [0.001,0.003]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
            "-test_accuracy\n",
            "-- [0.85714286 0.85714286 0.82142857 0.81481481 0.85185185 0.81481481\n",
            " 0.33333333]\n",
            "Média do test_accuracy: 0.7643613000755858\n",
            "Desvio 0.17687942914655935\n",
            "Intervalo [0.411,1.118]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
            "-test_precision_macro\n",
            "-- [0.85714286 0.88888889 0.8368984  0.82954545 0.85833333 0.81666667\n",
            " 0.27380952]\n",
            "Média do test_precision_macro: 0.7658978743012356\n",
            "Desvio 0.20206691046369302\n",
            "Intervalo [0.362,1.170]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
            "-test_recall_macro\n",
            "-- [0.85714286 0.85714286 0.82142857 0.81868132 0.8543956  0.81318681\n",
            " 0.34340659]\n",
            "Média do test_recall_macro: 0.7664835164835163\n",
            "Desvio 0.17365220482291785\n",
            "Intervalo [0.419,1.114]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos comparar e temos que:\n",
        "\n",
        "- regressão logística *sem otimização*:\n",
        "  - acuracia: 0,753\n",
        "  - precisão: 0,758\n",
        "  - recall: 0,755\n",
        "- regressão logística *utilizando grid search*:\n",
        "  - acuracia: 0,764\n",
        "  - precisão: 0,765\n",
        "  - recall: 0,766\n",
        "\n",
        "\n",
        "Podemos ver que nesse caso tivemos uma pequena melhora no desempenho."
      ],
      "metadata": {
        "id": "6QPEDIL-2kwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicando o random search na otimização de parametros"
      ],
      "metadata": {
        "id": "uBfMDCUX3hG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parametros2 = {'penalty':['l1', 'l2', 'elasticnet','none'],'dual':[True,False],'fit_intercept':[True,False],\n",
        "              'max_iter':np.arange(1,101,1),'multi_class':['auto','ovr','multinomial'],}"
      ],
      "metadata": {
        "id": "fnJ_TUgB3oB7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search = RandomizedSearchCV(estimator=LogisticRegression(),param_distributions=parametros2)\n",
        "random_search.fit(x_nm,y_nm)\n",
        "melhores_parametros2 = random_search.best_params_\n",
        "melhor_resultado2 = random_search.best_score_\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3ZIsRwc371t",
        "outputId": "f327d5c0-d7bf-4440-90bc-5ebc793a8045"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "40 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 453, in _check_solver\n",
            "    \"Solver %s supports only dual=False, got dual=%s\" % (solver, dual)\n",
            "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.73333333        nan\n",
            "        nan 0.70202429        nan        nan]\n",
            "  category=UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(melhores_parametros2)\n",
        "print(melhor_resultado2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKyHi2Y_4C3l",
        "outputId": "2a92f795-cf86-4082-ac95-d2b5889e1b97"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'penalty': 'none', 'multi_class': 'ovr', 'max_iter': 52, 'fit_intercept': True, 'dual': False}\n",
            "0.7333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lg2 = LogisticRegression(penalty='none',multi_class='ovr',max_iter=52,fit_intercept=True,\n",
        "                         dual=False,)\n",
        "\n",
        "metricas_ran2 = cross_validate(lg2,x_nm, y_nm, cv=7, scoring=nome_metricas)\n",
        "for met in metricas_ran:\n",
        "  print(f'-{met}')\n",
        "  print(f\"-- {metricas_ran2[met]}\")\n",
        "  media = np.mean(metricas_ran2[met])\n",
        "  desvio = np.std(metricas_ran2[met])\n",
        "  print(f'Média do {met}: {media}')\n",
        "  print(f'Desvio {desvio}')\n",
        "  print(f'Intervalo [{(media-(2*desvio)):.3f},{(media+(2*desvio)):.3f}]')\n",
        "  print('-*-'*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4XCghQi4f2k",
        "outputId": "64facb70-2010-40b4-eeab-c28d7e974983"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-fit_time\n",
            "-- [0.00647926 0.00335574 0.00322914 0.00325394 0.00394559 0.0031538\n",
            " 0.00396419]\n",
            "Média do fit_time: 0.003911665507725307\n",
            "Desvio 0.0010940308351234163\n",
            "Intervalo [0.002,0.006]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
            "-score_time\n",
            "-- [0.00318694 0.00182843 0.00173235 0.00169969 0.00189066 0.00173831\n",
            " 0.00174189]\n",
            "Média do score_time: 0.0019740377153669086\n",
            "Desvio 0.0004989176492378942\n",
            "Intervalo [0.001,0.003]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
            "-test_accuracy\n",
            "-- [0.85714286 0.89285714 0.82142857 0.81481481 0.85185185 0.81481481\n",
            " 0.2962963 ]\n",
            "Média do test_accuracy: 0.7641723356009071\n",
            "Desvio 0.19280137120242297\n",
            "Intervalo [0.379,1.150]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
            "-test_precision_macro\n",
            "-- [0.85714286 0.91176471 0.8368984  0.82954545 0.85833333 0.81666667\n",
            " 0.24642857]\n",
            "Média do test_precision_macro: 0.7652542835315943\n",
            "Desvio 0.21369608241960797\n",
            "Intervalo [0.338,1.193]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
            "-test_recall_macro\n",
            "-- [0.85714286 0.89285714 0.82142857 0.81868132 0.8543956  0.81318681\n",
            " 0.30494505]\n",
            "Média do test_recall_macro: 0.7660910518053374\n",
            "Desvio 0.19005990992032212\n",
            "Intervalo [0.386,1.146]\n",
            "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compararndo os resultados, temos que:\n",
        "\n",
        "- regressão logística *sem otimização*:\n",
        "  - acuracia: 0,753\n",
        "  - precisão: 0,758\n",
        "  - recall: 0,755\n",
        "- regressão logística *utilizando grid search*:\n",
        "  - acuracia: 0,764\n",
        "  - precisão: 0,765\n",
        "  - recall: 0,766\n",
        "- regressão logística *utilizando random search*:\n",
        "  - acuracia: 0,764\n",
        "  - precisão: 0,765\n",
        "  - recall: 0,766\n",
        "\n",
        "\n",
        "Podemos ver que aplicando a otimização teve uma melhora, entretanto, com o grid search e o random search tiveram praticamente os mesmos resultados."
      ],
      "metadata": {
        "id": "e4hWLmDe5PbM"
      }
    }
  ]
}